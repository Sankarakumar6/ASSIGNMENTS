{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYcCgZp6LXJk1mLmDgre6Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sankarakumar6/ASSIGNMENTS/blob/main/Disater_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RK83Cs28xlFY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "#pd.set_option('display.max_rows', 0)\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "#model selection\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#nlp libraries\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#ml\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "#metrics\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "#deep learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'CPU'\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "if tf.config.experimental.list_physical_devices('GPU'):\n",
        "    device = 'GPU'"
      ],
      "metadata": {
        "id": "XFCuIMxcx30d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print the current library versions\n",
        "packages = [np, pd, mpl, sns, spacy]\n",
        "\n",
        "print('-'*35)\n",
        "for package in packages:\n",
        "    print(package.__name__, 'version:', package.__version__)\n",
        "    print('-'*35)"
      ],
      "metadata": {
        "id": "6lwKplUrx-S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paths = []\n",
        "for root, dirs, files in os.walk('/kaggle/input'):\n",
        "    for file in sorted(files):\n",
        "        paths.append(os.path.join(root, file))\n",
        "print(paths)"
      ],
      "metadata": {
        "id": "VpxNZn-8x-Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(paths[2], index_col='id')\n",
        "test_df = pd.read_csv(paths[1], index_col='id')\n",
        "concat = [train_df, test_df]"
      ],
      "metadata": {
        "id": "W8UHmt43x-Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how large is the corpus\n",
        "print('CORPUS')\n",
        "print('Training set corpus size:', train_df.shape[0])\n",
        "print('Test set corpus size:', test_df.shape[0])"
      ],
      "metadata": {
        "id": "a4qccP61x-KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# keyword and location are apparently missing in many cases\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "18pkmpfux-H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we will have to deal with keyword and location null values\n",
        "print('-'*50)\n",
        "print('\\nTRAINING DATA:\\n')\n",
        "print(train_df.info())\n",
        "print('-'*50)\n",
        "print('\\nTESTING DATA:\\n')\n",
        "print(test_df.info())"
      ],
      "metadata": {
        "id": "dTO224y2x-FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#how do keywords look like?\n",
        "#there are 221 unique values\n",
        "train_df['keyword'].value_counts()"
      ],
      "metadata": {
        "id": "L1R6evA0x-Cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how do locations look like?\n",
        "# there are a lot unique values, which probably don't have a lot of predictive power\n",
        "train_df['location'].value_counts()"
      ],
      "metadata": {
        "id": "TWtT0yfxx-AM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets look at some regex examples\n",
        "example = 'The quick brown fox jumps over the lazy dog.'\n",
        "# find the word jumps\n",
        "print(re.findall(r'j\\w+', example))\n",
        "# find words that are exactly 4 letters long\n",
        "print(re.findall(r'\\b\\w{4}\\b', example))\n",
        "# find pairs of words\n",
        "print(re.findall(r'\\w+\\s\\w+', example))\n",
        "# find the last word in the sentence\n",
        "print(re.findall(r'\\w+.?$', example))"
      ],
      "metadata": {
        "id": "ZbO_3syix993"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for df in concat:\n",
        "    # find a hashtag and create a new column\n",
        "    df['hashtags'] = df.text.str.findall(r'#\\w+')\n",
        "    # find a user mentioned and create a new column\n",
        "    df['user'] = df.text.str.findall(r'@\\w*')"
      ],
      "metadata": {
        "id": "hzdPAAXkx97N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "TXPd6oe5x92p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how many hashtags are in a tweet?\n",
        "for df in concat:\n",
        "    df['hash_count'] = df['hashtags'].str.len()"
      ],
      "metadata": {
        "id": "uL-afGGByYik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "HEn30FalyYfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# most tweets don't actually have a hashtag\n",
        "plt.figure(figsize=(20,10))\n",
        "sns.countplot(x='hash_count', data=train_df).set_title('Count for number of #hashtags')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mguDzqISyYdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# it does not look like people necessarily use more hashtags in case of emmergency\n",
        "plt.figure(figsize=(20,10))\n",
        "sns.barplot(x='hash_count', y='target', data=train_df).set_title('Number of hashtags vs % of real disaster tweets')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F9vZKSX4yYau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#how many of the tweets are actual disater tweets\n",
        "sns.countplot(x='target', data=train_df).set_title('Not Disaster vs Disater Tweets')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RuCTeI1PyYX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#look at the tweets themselves\n",
        "#some of the text seems to repeat\n",
        "train_df.describe(include=object)"
      ],
      "metadata": {
        "id": "rxseWhxzyYV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for df in concat:\n",
        "    df['length'] = pd.Series(df.loc[:, 'text'].str.len())\n",
        "    df['length'] = pd.Series(df.loc[:, 'text'].str.len())"
      ],
      "metadata": {
        "id": "sIr6vNCpyYTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(25, 10))\n",
        "axes = sns.countplot(x='length', data=train_df)\n",
        "axes.set_title('Length of tweet and disaster')\n",
        "axes.set_xticklabels(axes.get_xticklabels(), rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JxLlFAE5yYQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#deal with missing hashtags\n",
        "for df in concat:\n",
        "    df['hashtags'] = df['hashtags'].apply(lambda x: ['NoHashTag'] if not x else x)\n",
        "    df['hashtags'] = df['hashtags'].apply(lambda x: ' '.join(x))"
      ],
      "metadata": {
        "id": "N0PGplVhyYOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#deal with missing location and keyword\n",
        "for df in concat:\n",
        "    df['keyword'].fillna(value='Missing', inplace=True)\n",
        "    df['location'].fillna(value='Missing', inplace=True)"
      ],
      "metadata": {
        "id": "pwbXEn-3yYLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head()"
      ],
      "metadata": {
        "id": "l2k4eqsZyYJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clean text\n",
        "#lets write a function, as we probably have to do several cleaning steps\n",
        "#there are probably better solutions out there\n",
        "#TODO look at other notebooks\n",
        "def clean_tweet(doc):\n",
        "\n",
        "    #remove urls\n",
        "    doc =  re.sub(r'https?://\\w?\\.?\\w*/\\w*', '', doc)\n",
        "    #remove @user\n",
        "    doc = re.sub(r'@\\w*', '', doc)\n",
        "    #remove date in the format nn/nn/nn\n",
        "    doc = re.sub(r'\\d+/\\d+/\\d+', '', doc)\n",
        "    #remove time in the format hh:mm\n",
        "    doc = re.sub(r'\\d+:\\d+', '', doc)\n",
        "    #remove special signs\n",
        "    doc = re.sub(r'[#@.?:-=/\\\\<>\\]\\[]', '', doc)\n",
        "    #remove words containing numbers\n",
        "    doc = re.sub(r'(\\w+\\d+|\\d+\\w+)', '', doc)\n",
        "    return doc\n",
        "\n",
        "for df in concat:\n",
        "    df['text'] = df['text'].apply(clean_tweet)"
      ],
      "metadata": {
        "id": "OvxdO-BxyYGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "g1E-r6mXyYD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1-gram tokenizer\n",
        "example = 'The quick brown fox jumps over the lazy dog.'\n",
        "\n",
        "# remove the dots and make all words lower case\n",
        "clean_example = re.sub(r'\\.', '', example).lower()\n",
        "print(clean_example.split())"
      ],
      "metadata": {
        "id": "fllsMb47y8Pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2-gram tokenizer\n",
        "\n",
        "example = 'The quick brown fox jumps over the lazy dog.'\n",
        "\n",
        "without_first = example.split()[1:]\n",
        "without_last = example.split()[:-1]\n",
        "\n",
        "list(zip(without_last, without_first))"
      ],
      "metadata": {
        "id": "L_XkFqN1y8MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a simple stemmer that only takes care of the trailing s could for example look like this\n",
        "def stem(sentence):\n",
        "    stemmed_sentence = []\n",
        "    for word in sentence.split():\n",
        "        stemmed_word = re.findall(r'^(.*?)(s)?$', word)[0][0]\n",
        "        stemmed_sentence.append(stemmed_word)\n",
        "    return ' '.join(stemmed_sentence)\n",
        "\n",
        "# the stemmer works pretty good on this example\n",
        "example = 'The quick brown fox jumps over the lazy dog.'\n",
        "print(stem(example))\n",
        "\n",
        "# a lot of meaning is lost with this stemmer\n",
        "example_2 = 'He was on the bus with his abs'\n",
        "print(stem(example_2))"
      ],
      "metadata": {
        "id": "HJkJskwIy8J6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I can imagine, that for very simple tools there is a simple lookup table\n",
        "lemma_lookup = {'go': 'go',\n",
        "               'went': 'go',\n",
        "               'goes': 'go',\n",
        "               'gone': 'go',\n",
        "               'going': 'go',\n",
        "               'jumps': 'jump',\n",
        "               'jumped': 'jump',\n",
        "               'jumping': 'jump'}\n",
        "\n",
        "def lemma(sentence):\n",
        "    for word in sentence.lower().split():\n",
        "        print(lemma_lookup.get(word, word))\n",
        "\n",
        "example = 'The quick brown fox jumps over the lazy dog.'\n",
        "lemma(example)"
      ],
      "metadata": {
        "id": "LxELTIZay8Ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = ['the', 'over', 'by', 'to', 'from']\n",
        "example = 'The quick brown fox jumps over the lazy dog.'\n",
        "\n",
        "#removing stop words is really simple with a list comprehension\n",
        "[word for word in example.lower().split() if word not in stop_words]"
      ],
      "metadata": {
        "id": "gj7XDpd6y8E2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example of one hot encoding\n",
        "print('-' * 100)\n",
        "\n",
        "sentence = 'the quick brown fox jumps over the lazy dog'\n",
        "row_lookup = {}\n",
        "\n",
        "print('Origninal sentence: {}'.format(sentence))\n",
        "print('-' * 100)\n",
        "\n",
        "# unique words in the corpus represent the number of rows in the matrix\n",
        "row_names = set(sentence.split())\n",
        "for i, row in enumerate(row_names):\n",
        "    row_lookup[row] = i\n",
        "\n",
        "\n",
        "rows = len(row_names)\n",
        "print('Row Encodings: ', row_lookup)\n",
        "print('Column Encodings: ', sentence)\n",
        "# the length of the sentence is the number of columns\n",
        "columns = len(sentence.split())\n",
        "\n",
        "print('-' * 100)\n",
        "one_hot = np.zeros((rows, columns))\n",
        "\n",
        "for i, column in enumerate(sentence.split()):\n",
        "    one_hot[row_lookup[column], i] = 1\n",
        "\n",
        "print(one_hot)\n",
        "\n",
        "print('-' * 100)"
      ],
      "metadata": {
        "id": "zpye-X0Py8Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('Second bag of words')\n",
        "print(bag_2)\n",
        "print('-'*120)#creating bag of words\n",
        "from collections import Counter\n",
        "words = []\n",
        "\n",
        "sentence_1 = 'the quick brown fox jumps over the lazy dog'\n",
        "sentence_2 = 'other word'\n",
        "\n",
        "words_1 = [word for word in sentence_1.split()]\n",
        "words_2 = [word for word in sentence_2.split()]\n",
        "unique_words = set(words_1 + words_2)\n",
        "\n",
        "counter_1 = Counter(words_1)\n",
        "counter_2 = Counter(words_2)\n",
        "\n",
        "bag_1 = {}\n",
        "bag_2 = {}\n",
        "for word in unique_words:\n",
        "    bag_1[word] = counter_1[word]\n",
        "    bag_2[word] = counter_2[word]\n",
        "\n",
        "print('-'*120)\n",
        "print('First bag of words')\n",
        "print(bag_1)\n",
        "print('-'*120)"
      ],
      "metadata": {
        "id": "6CQhvs4fy8Ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate tf idf with pandas\n",
        "# lets look at the following sentences to understand TF-IDF\n",
        "sentence_1 = 'the quick brown fox jumps over the lazy dog'\n",
        "sentence_2 = 'the lazy and dirty dog enjoys his meal'\n",
        "sentence_3 = 'the fox hunts and eats the chicken'\n",
        "sentence_4 = 'the angry hunter wants to avenge the chicken'\n",
        "sentence_5 = 'the hunter can`t find the fox and punishes the dog'\n",
        "sentence_6 = 'the dog is hungry'\n",
        "sentence_7 = 'the dog eats the fox'\n",
        "\n",
        "#will be used for pandas df index\n",
        "index=['sentence_1', 'sentence_2', 'sentence_3', 'sentence_4', 'sentence_5', 'sentence_6', 'sentence_7']\n",
        "\n",
        "#for easier looping\n",
        "corpus=[sentence_1, sentence_2, sentence_3, sentence_4, sentence_5, sentence_6, sentence_7]\n",
        "\n",
        "#'document' : Counter('token: count_in_document')\n",
        "counters = {}\n",
        "for idx, document in enumerate(corpus):\n",
        "    counters[index[idx]] = Counter(document.split())\n",
        "\n",
        "df = pd.DataFrame(counters).transpose()\n",
        "\n",
        "row_sum = df.sum(axis=1)\n",
        "col_count = df.count(axis=0)\n",
        "df.fillna(value=0, inplace=True)\n",
        "\n",
        "#term frequency\n",
        "tf = df.div(row_sum, axis='index')\n",
        "#inverse document frequency\n",
        "idf = np.log(len(corpus) / col_count)\n",
        "\n",
        "#finally tf-idf\n",
        "tf_idf = tf * idf\n",
        "tf_idf"
      ],
      "metadata": {
        "id": "D_8k8IpXy7-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can use numpy to calculate similarity between the TF-IDF representaion\n",
        "def similarity(doc_1, doc_2):\n",
        "    #numerator\n",
        "    num = np.dot(doc_1, doc_2)\n",
        "    #denominator\n",
        "    norm_a = np.linalg.norm(doc_1)\n",
        "    norm_b = np.linalg.norm(doc_2)\n",
        "    den = norm_a * norm_b\n",
        "    return num * den\n",
        "\n",
        "# sentence 1 and 4 are very dissimilar (no common words)\n",
        "print(similarity(tf_idf.loc['sentence_1', :], tf_idf.loc['sentence_4', :]))\n",
        "\n",
        "# sentence 1 and 7 are more similar, because the both have dog and fox mentioned\n",
        "print(similarity(tf_idf.loc['sentence_1', :], tf_idf.loc['sentence_7', :]))"
      ],
      "metadata": {
        "id": "CQcmjrVLy78A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " We start by working with dummy corpus to learn the libraries\n",
        "# We introduce some punctuation e.t.c. to test how good the libraries are\n",
        "corpus = [\n",
        "    'The quick brown fox jumps over the lazy dog.',\n",
        "    'The lazy and dirty dog enjoys his meal.',\n",
        "    'The fox hunts and eats the chicken.',\n",
        "    'The angry hunter wants to avenge the chicken.',\n",
        "    'The hunter can`t find the fox and punishes the dog.',\n",
        "    'The dog is hungry!!!',\n",
        "    \"The dog doesn't let the fox hunt a chicken again.\",\n",
        "    'The hunter went to the doctor!!!'\n",
        "]\n",
        "\n",
        "corpus"
      ],
      "metadata": {
        "id": "oJNgAfoXy75p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spaCy\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "G-R8DIMkzlMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(corpus[0])\n",
        "\n",
        "#at first clance this is just a sentence\n",
        "print(doc)\n",
        "\n",
        "#But we receive a Doc class\n",
        "#According to spaCy documentation \"A Doc is a sequence of Token objects\"\n",
        "print(type(doc))"
      ],
      "metadata": {
        "id": "2o6-kHolzlI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets loop over the doc object\n",
        "\n",
        "#https://spacy.io/usage/linguistic-features\n",
        "    #Text: The original word text.\n",
        "    #Lemma: The base form of the word.\n",
        "    #POS: The simple UPOS part-of-speech tag.\n",
        "    #Tag: The detailed part-of-speech tag.\n",
        "    #Dep: Syntactic dependency, i.e. the relation between tokens.\n",
        "    #Shape: The word shape â€“ capitalization, punctuation, digits.\n",
        "    #is alpha: Is the token an alpha character?\n",
        "    #is stop: Is the token part of a stop list, i.e. the most common words of the language?\n",
        "\n",
        "for token in doc:\n",
        "    print('TOKEN: ', token.text, '\\t', '| LEMMA: ', token.lemma_, '   \\t', '| POS: ', token.pos_, '\\t' \\\n",
        "          '| TAG: ', token.tag_, '\\t',  '| DEPENDENCY: ', token.dep_, '     \\t', \\\n",
        "          '| SHAPE: ',  token.shape_, '   \\t', 'ISALPHA: ', token.is_alpha, '\\t', 'ISSTOPWORD: ', token.is_stop)"
      ],
      "metadata": {
        "id": "42jDlA6ozlGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spacy has an explain method to help with the language terminology\n",
        "spacy.explain('ADJ')"
      ],
      "metadata": {
        "id": "Wd7ybM45zlEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets take it one step at a time and look at a simple sentence\n",
        "print(corpus[-1:])\n",
        "\n",
        "doc = nlp(corpus[-1])\n",
        "\n",
        "print('\\nLEMMAS:')\n",
        "print('-'*30)\n",
        "# How do the lemmas look like?\n",
        "# The tokens look natural. And it correctly transforms 'went' into 'go'\n",
        "for token in doc:\n",
        "    print(token.lemma_, end=\" \")\n",
        "\n",
        "# What are the stop words\n",
        "print('\\n\\nSTOP WORDS:')\n",
        "print('-'*30)\n",
        "for token in doc:\n",
        "    if token.is_stop: print(token.text, end=\" \")"
      ],
      "metadata": {
        "id": "YmmlPYP6zlBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# an additional advantage of spacy is that it allows you to draw the relationships in a sentence\n",
        "displacy.render(doc, style=\"dep\")"
      ],
      "metadata": {
        "id": "L96Xhfuhzk_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets reduce the pipeline\n",
        "# we don't need all component of spacy, removing parts in the pipeline improves the performance\n",
        "# https://spacy.io/usage/processing-pipelines#pipelines\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"textcat\"])\n",
        "\n",
        "#nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "J-oB-QJszk88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspiration for the function\n",
        "# https://towardsdatascience.com/turbo-charge-your-spacy-nlp-pipeline-551435b664ad\n",
        "def lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    #remove stop words and punctuation and return the lemmas ow words\n",
        "    return ' '.join([token.lemma_.lower() for token in doc if not token.is_stop and not token.pos_ == 'PUNCT'])\n",
        "\n",
        "for df in concat:\n",
        "    df['lemma'] = df['text'].apply(lemmatize)"
      ],
      "metadata": {
        "id": "7j2V9SILzk6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# looks reasonable enough\n",
        "train_df.loc[:, ['text', 'lemma']].head(10)"
      ],
      "metadata": {
        "id": "_1v_q3Pazk4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we use the TF-IDF Class to generate the vectorized form of the corpus\n",
        "vectorizer = TfidfVectorizer()\n",
        "corpus_train = train_df.loc[:, 'lemma']\n",
        "corpus_test = test_df.loc[:, 'lemma']\n",
        "\n",
        "# I don't quite understand how the fit function would work for the test data if we fit the data on the training set\n",
        "# The TF has to be calculated for each document, but the IDF has to be calculated based on each word and the documents that the word appears\n",
        "# do we use the test data, the test + train data or only the train data to calculate the idf (I assume train data)\n",
        "# if we fit the model on the training data I would assume, that only the training data is used, but how is dealt with new words\n",
        "\n",
        "#building a pipeline in sklearn might be a good idea overall\n",
        "X_train = vectorizer.fit_transform(corpus_train)\n",
        "X_test = vectorizer.transform(corpus_test)"
      ],
      "metadata": {
        "id": "FaxaKCT1zk2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# there are 7613 sentences and 12394 words\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "usgn_-vhzkzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here we can get all the available words\n",
        "#print(vectorizer.get_feature_names())"
      ],
      "metadata": {
        "id": "oMPapbT3zkxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train_df['target']"
      ],
      "metadata": {
        "id": "FG6zxQygzku4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression could be used as a baseline\n",
        "log_reg = LogisticRegression()\n",
        "scores = cross_val_score(log_reg, X_train, y_train, cv=10, scoring='f1', n_jobs=-1)"
      ],
      "metadata": {
        "id": "Sx-rvxWuzksk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(scores)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "id": "jBO8aQXxzkqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the model on all data points for submission\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "LsdeQ0kFzkoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_train, log_reg.predict(X_train))"
      ],
      "metadata": {
        "id": "b5iIohBE0LF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression could be used as a baseline\n",
        "knn = KNeighborsClassifier(n_neighbors=100)\n",
        "scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='f1', n_jobs=-1)"
      ],
      "metadata": {
        "id": "CWSWWt0k0LC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(scores)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "id": "Sm74SrW30LAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(n_estimators=100)\n",
        "#this part takes a lot of time if you use cv\n",
        "# scores = cross_val_score(rf, X_train, y_train, cv=10, scoring='f1', n_jobs=-1)\n",
        "rf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "bRrNu3hl0K9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_train, rf.predict(X_train))"
      ],
      "metadata": {
        "id": "0pAWWgJd0K7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SOME PARAMETERS OF XGBOOST\n",
        "# eta = learning_rate (default=0.3)\n",
        "# gamma = min_split_loss (default=0)\n",
        "# objective = loss_function (default=reg:squarederror), we will use binary:logistic\n",
        "\n",
        "#here we create a cross val cross validation set in order to be able to use early stopping\n",
        "X_train_small, X_cv, y_train_small, y_cv = train_test_split(X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)\n",
        "\n",
        "\n",
        "#we determined at the top if we are using the gpu\n",
        "#if device == 'GPU':\n",
        "    # better turn on the gpu\n",
        "    #xgb_clf = xgb.XGBClassifier(n_estimators=1000, gamma=0.1, objective='binary:logistic', tree_method='gpu_hist')\n",
        "\n",
        "    #xgb_clf.fit(X_train_small, y_train_small, eval_set=[(X_cv, y_cv)], eval_metric=\"logloss\", verbose=True, early_stopping_rounds=10)\n",
        "\n",
        "    # in case we want to use stratified cross validation\n",
        "    #scores = cross_val_score(xgb_clf, X_train, y_train, cv=10, scoring='f1')"
      ],
      "metadata": {
        "id": "DEUm2HWE0KwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(scores)\n",
        "#print(scores.mean())"
      ],
      "metadata": {
        "id": "jprSEynm0Kst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if device == 'GPU':\n",
        "#    #xgb_clf.fit(X_train, y_train)\n",
        "#    f1_score(y_train, xgb_clf.predict(X_train))"
      ],
      "metadata": {
        "id": "5E_RWfcY0f8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_df.text)"
      ],
      "metadata": {
        "id": "-hMYiVmZ0f5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# over 15000 unique tokens\n",
        "# that is probably too much for such a small dataset size\n",
        "print(len(tokenizer.word_index))"
      ],
      "metadata": {
        "id": "b3KtUWBd0f3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "nO3Y7qGA0f0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# keras model\n",
        "#mlp_model = keras.models.Sequential()\n",
        "#mlp_model.add(layers.Input(shape=(X_train.shape[1],)))\n",
        "#mlp_model.add(layers.Dense(100, activation='relu'))\n",
        "#mlp_model.add(layers.Dense(20, activation='relu'))\n",
        "#mlp_model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "fsq-1VIw0fyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)"
      ],
      "metadata": {
        "id": "q9hGxpE60fuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "HctOjJ870frE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#history = mlp_model.fit(X_train, y_train, epochs=200, batch_size=128, validation_split=0.25, callbacks=[callback])"
      ],
      "metadata": {
        "id": "MRkNjxmD0foa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# keras model\n",
        "'''\n",
        "embed_model = keras.models.Sequential()\n",
        "#embed_model.add(layers.Input(shape=(X_train.shape[1],)))\n",
        "embed_model.add(layers.Embedding(X_train.shape[1], 100, input_length=X_train.shape[1]))\n",
        "embed_model.add(layers.Flatten())\n",
        "embed_model.add(layers.Dense(100, activation='relu'))\n",
        "embed_model.add(layers.Dense(10, activation='relu'))\n",
        "embed_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "'''"
      ],
      "metadata": {
        "id": "n9wkyK_70fl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)"
      ],
      "metadata": {
        "id": "5KleV9ZP0fjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#embed_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "-CGBk4N30fhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#history = embed_model.fit(X_train, y_train, epochs=200, batch_size=128, validation_split=0.25, callbacks=[callback])"
      ],
      "metadata": {
        "id": "bRoHLz_Z0fe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "rnn_model = keras.models.Sequential()\n",
        "rnn_model.add(layers.Input(shape=(None, X_train.shape[1])))\n",
        "#rnn_model.add(layers.Embedding(X_train.shape[1], 10))\n",
        "rnn_model.add(layers.SimpleRNN(10))\n",
        "rnn_model.add(layers.Dense(10, activation='relu'))\n",
        "rnn_model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "JyQ5mAHr0fb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "LkOu12xt0fZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 5000\n",
        "embedding_dim = 100\n",
        "maxlen = 100\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train_df.text)"
      ],
      "metadata": {
        "id": "F376qr4X08I4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = tokenizer.texts_to_sequences(train_df.text)\n",
        "X_test = tokenizer.texts_to_sequences(test_df.text)"
      ],
      "metadata": {
        "id": "zugWUOHI08Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "0j5g-2GR08C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "WAk9MZnc07xZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)"
      ],
      "metadata": {
        "id": "cMG_cSLO07ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "lstm_model = keras.models.Sequential()\n",
        "lstm_model.add(layers.Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "lstm_model.add(layers.LSTM(32,return_sequences=True))\n",
        "lstm_model.add(layers.LSTM(32,return_sequences=True))\n",
        "lstm_model.add(layers.LSTM(32,return_sequences=True))\n",
        "lstm_model.add(layers.LSTM(32))\n",
        "lstm_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "'''"
      ],
      "metadata": {
        "id": "LSCkDhb407sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lstm_model.compile(optimizer=keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "PnOA3mnc07pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "cnn_model = keras.models.Sequential()\n",
        "cnn_model.add(layers.Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "cnn_model.add(layers.Conv1D(16, 7, activation='relu'))\n",
        "cnn_model.add(layers.Dropout(0.5))\n",
        "cnn_model.add(layers.MaxPooling1D(5))\n",
        "cnn_model.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "cnn_model.add(layers.Dropout(0.5))\n",
        "cnn_model.add(layers.GlobalMaxPooling1D())\n",
        "cnn_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "'''"
      ],
      "metadata": {
        "id": "cEiQtPSM07nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "MecO532U07lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tfhub\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "hub_model = keras.Sequential([\n",
        "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\", dtype=tf.string, input_shape=[], output_shape=[50]),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "dg4SvBWS07iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hub_model.summary()"
      ],
      "metadata": {
        "id": "zfs_GO4w07fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hub_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "kYTNptj607cQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_df['text'].to_numpy()\n",
        "y_train = train_df['target'].to_numpy()"
      ],
      "metadata": {
        "id": "46g8_uP81b4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = test_df['text'].to_numpy()"
      ],
      "metadata": {
        "id": "VamZAPnv1b0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "wrg5yAZH1bxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tr = X_train[:6000]\n",
        "X_train_cv = X_train[6000:]\n",
        "\n",
        "y_train_tr = y_train[:6000]\n",
        "y_train_cv = y_train[6000:]"
      ],
      "metadata": {
        "id": "klKQxnkG1buw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((X_train_tr, y_train_tr))\n",
        "validation_data = tf.data.Dataset.from_tensor_slices((X_train_cv, y_train_cv))"
      ],
      "metadata": {
        "id": "jsvN6YEO1bsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "SIj0HABk1bpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_examples_batch, train_labels_batch = next(iter(train_dataset.batch(10)))"
      ],
      "metadata": {
        "id": "mtaioAry1prf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_examples_batch"
      ],
      "metadata": {
        "id": "yygD_YTx1ppS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = hub_model.fit(train_data.shuffle(10000).batch(512),\n",
        "                    epochs=20,\n",
        "                    verbose=1,\n",
        "                    validation_data=validation_data.batch(512))"
      ],
      "metadata": {
        "id": "1BFq6Phf1plT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = hub_model\n",
        "\n",
        "# for sklearn\n",
        "#y_pred = model.predict(X_test)\n",
        "\n",
        "# for keras\n",
        "y_pred = model.predict_classes(X_test)\n",
        "y_pred = y_pred.reshape(y_pred.shape[0],)\n",
        "\n",
        "submit_df = pd.DataFrame({'id': test_df.index, 'target': y_pred})"
      ],
      "metadata": {
        "id": "y6-_cesW1pjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_df.shape"
      ],
      "metadata": {
        "id": "1aT-GTel1pgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_df.head()"
      ],
      "metadata": {
        "id": "cegooN3P1pdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_df.to_csv('submission.csv', index=False)\n",
        "print(\"Your submission was successfully saved!\")"
      ],
      "metadata": {
        "id": "n7SdTuEC1paD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bsLFz3P31pXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CQLYDgpO1pU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P8hBzlmi1pR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6nNGGDRH1pPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KXuUuECX1pMc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}